{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"<code>lightcurver</code> Documentation","text":""},{"location":"#what-is-lightcurver","title":"What is <code>lightcurver</code>?","text":"<p><code>lightcurver</code> is a package / pipeline leveraging <code>STARRED</code> for precise photometry of blended sources in a large number of epochs. It specifically tackles the following type of problem:</p> <ul> <li>The dataset has a large number of frames.</li> <li>The frames are oversampled wide-field images (think modern sky surveys, such as the upcoming Vera Rubin LSST).</li> <li>Only a small portion of each frame if of interest (think supernova embedded in a galaxy, or lensed quasars).</li> </ul> <p>The above type of problem has historically been hard to solve: obtaining high quality light curves for such datasets has mostly been a manual process, taking up to a few months of an investigator's time per light curve. Doubt would also always remain: have we extracted as much signal as possible from the data, or could the signal-to-noise ratio of the curves be improved if we re-process everything just one last time? Are there systematics errors in the normalization that bias the shape of the extracted light curves?</p> <p><code>lightcurver</code> aims at making the process at least semi-automatic (set it up once for a few frames, then let the pipeline automatically handle any future frame), and builds up on experience of manually reducing such datasets to offer  a no-compromise photometric precision. The end goal is being able to claim that the photometric uncertainty obtained in the light curves is dominated by  the noise levels in the original frames, and not by normalisation or deblending scatter or systematics.</p>"},{"location":"#example-result","title":"Example result","text":"<p>The two figures below show <code>lightcurver</code> outputs for a dataset captured by the ESO 2.2 meters telescope. (1)</p> <ol> <li>ESO program <code>0106.A-9005(A)</code>, PI Courbin, La Silla Chile</li> </ol> <p>The first image shows a calibrated cutout of the object of interest, a lensed quasar, from one of the wide-field images of the dataset. The second image is the <code>STARRED</code> model, a high resolution image cumulating the signal of all the frames. The last image is a Hubble Space Telescope image for comparison.</p> <p></p> <p>The other product being the lightcurves of the point sources:</p> <p></p>"},{"location":"#the-method","title":"The method","text":"<p><code>lightcurver</code> will prepare, for each frame, a precisely flux-calibrated cutout of the region of interest, together with a ready-to-use PSF model. The cutout pixels can then be modelled with <code>STARRED</code>, in a so-called \"joint-forward-modelling\". The process fits a high resolution model, with a constant pixelated component and a variable-flux point source component, to all available epochs at once, minimizing one global chi-squared. This allows us to both obtain precise light curves of the point sources modelled with the PSF, and the high resolution model that cumulates the signal of al the frames of the region of interest.</p>"},{"location":"cookbook/","title":"Cookbook and random fixes","text":"<p><code>lightcurver</code> will absolutely fail you a lot. Sorry, astronomical data is just too messy and such is life. Here I will add example situations and how to fix them.</p>"},{"location":"cookbook/#some-of-my-images-were-imported-but-cannot-be-plate-solved-due-to-low-quality","title":"Some of my images were imported, but cannot be plate solved due to low quality","text":"<p>High airmass observations, clouds, tracking problems ... If you have such images and are confident that you will not be able to extract value from them,  you can remove them from consideration by flagging them in the database: <pre><code>'UPDATE frames SET comment='cannot be plate solved', eliminated = 1 WHERE plate_solved=0;'\n</code></pre></p>"},{"location":"cookbook/#i-manually-plate-solved-my-images-after-importation-how-can-i-make-my-pipeline-aware-of-this","title":"I manually plate solved my images after importation, how can I make my pipeline aware of this?","text":"<p>If my plate solving step failed you, and if you managed to add a WCS yourself to the fits files in your <code>frames</code> directory, then you will need to manually run the process storing the footprints in the database and checking that your region of interest is in the frame. Here is how you might do that, with your current directory set to your working directory. <pre><code>import os\nos.environ['LIGHTCURVER_CONFIG'] = \"/path/to/config.yaml\"\nfrom pathlib import Path\nfrom astropy.io import fits\nfrom astropy.wcs import WCS\n\nfrom lightcurver.processes.plate_solving import post_plate_solve_steps\nfrom lightcurver.structure.user_config import get_user_config\nfrom lightcurver.structure.database import execute_sqlite_query\nfrom lightcurver.processes.frame_star_assignment import populate_stars_in_frames\n\nuser_config = get_user_config()\n\nsolved = Path('frames').glob('*.fits')\n\nfor s in solved:\n    s = str(s)\n    if 'sources.fits' in s:\n        # this is a table of sources, skip\n        continue\n    wcs = WCS(fits.getheader(s))\n    if not wcs.is_celestial:\n        # this one wasn't solved then\n        continue\n    frame_id = execute_sqlite_query('select id from frames where image_relpath = ?', \n                                    params=(s,), is_select=True)[0][0]\n\n    try:\n       post_plate_solve_steps(frame_path=s, user_config=user_config, frame_id=frame_id)\n    except AssertionError:\n         # already inserted\n        pass\n\n    execute_sqlite_query(query=\"UPDATE frames SET plate_solved = ? WHERE id = ?\",\n                         params=(1, frame_id), is_select=False)\n\n# now that we know what our footprints are, populate the table telling us which frame has which star.\npopulate_stars_in_frames()\n</code></pre></p>"},{"location":"cookbook/#the-sources-were-not-correctly-found-by-sep-how-to-re-run-that-part-only-after-changing-the-config","title":"The sources were not correctly found by <code>sep</code>, how to re-run that part only after changing the config?","text":"<p><pre><code>import os\nos.environ['LIGHTCURVER_CONFIG'] = \"/path/to/config.yaml\"\n\nfrom lightcurver.pipeline.task_wrappers import source_extract_all_images\n\nsource_extract_all_images()\n</code></pre> You can also pass a list of strings to <code>source_extract_all_images</code>, filtering on the <code>frames</code> table of the database,  for instance: <pre><code>source_extract_all_images(['plate_solved = 0'])\n</code></pre></p>"},{"location":"installation/","title":"Installation","text":"<p><code>lightcurver</code> requires several components to function:</p> <ul> <li><code>STARRED</code>: it will be installed automatically with <code>pip install lightcurver</code>, but if you want to use it with a GPU there might be some more setup to do. See the installation instructions of the package itself.</li> <li>The dependencies handled by your python package manager, such as <code>astropy</code>, <code>shapely</code>, <code>astroquery</code>, <code>pandas</code>, <code>pyyaml</code>  ...these will be installed automatically by <code>pip install lightcurver</code>.</li> <li>(optional) <code>Astrometry.net</code>: their installation instructions should get you started.  Alternatively, you can get an API key from their nova service. I would recommend against using it in production, as to not overload their servers.</li> </ul> <p>So, I would suggest creating a python (3.9+, ideally 3.11) environment, say <code>lightcurver_env</code>, and install the present package in it.</p>"},{"location":"installation/#the-quick-version","title":"The quick version","text":"<p>Chances are this will work: <pre><code>    conda activate lightcurver_env # if using conda\n    source lightcurver_env/bin/activate # if using python's venv\n    pip install lightcurver\n</code></pre></p> <p>Or for the <code>git</code> version (includes some minimal test data): <pre><code>    git clone git@github.com:duxfrederic/lightcurver.git\n    cd lightcurver\n    conda activate lightcurver_env\n    pip install .\n</code></pre></p>"},{"location":"installation/#if-the-quick-version-fails-list-of-dependencies","title":"If the quick version fails: list of dependencies","text":"<p>Should the above fail, there might be a dependency problem requiring the manual handling of the different packages.  Here is the list of dependencies that need be installed:</p> <ol> <li><code>numpy &lt; 2.00</code> - as of June 2024, <code>sep</code> is not compatible with <code>numpy &gt;= 2.00</code></li> <li><code>scipy</code></li> <li><code>matplotlib</code></li> <li><code>pandas</code></li> <li><code>astropy</code></li> <li><code>astroquery</code> - for querying Gaia and VizieR</li> <li><code>h5py</code> - for storing cutouts and PSF models</li> <li><code>photutils</code> - for aperture photometry used as initial guess</li> <li><code>astroalign</code> - for finding transformations between frames</li> <li><code>shapely</code> - for computing footprints of frames</li> <li><code>ephem</code> - for calculating airmass, moon distance, etc.</li> <li><code>pytest</code> - for executing the automatic tests</li> <li><code>sep</code> - for source and background extraction</li> <li><code>astroscrappy</code> - for cleaning the cosmics</li> <li><code>pyyaml</code> - for reading the config file</li> <li><code>starred-astro</code> - assume the latest version, will install its own dependencies.</li> <li><code>widefield_plate_solver</code> - an astrometry.net wrapper</li> <li><code>ccdproc</code> - for identifying bad columns of pixels</li> </ol>"},{"location":"installation/#testing-your-installation","title":"Testing your installation","text":"<p>You can test your installation by following the tutorial. The automated tests also include the processing of a subset of the dataset given in the tutorial, you can thus run them instead to check functionality (should take 1-2 minutes).  <pre><code>cd /your/clone/of/lightcurver\npytest .\n</code></pre></p> <p>If you are going to use a local installation of <code>Astrometry.net</code>, do not forget to download their index files as well! The combination of 4100 and 5200 should do the trick.</p>"},{"location":"tutorial/","title":"<code>lightcurver</code> tutorial","text":"<ol> <li>Think twice about this value: it has to contain enough stars, but not too many either. Aim for ~20 stars, look at one  of your images and do a rough inventory of what is available within a certain distance from your region of interest.</li> <li>You can set this to false if you are going to do the reduction yourself, or if you do not want the pipeline to waste time redoing this everytime. (This is not an incremental step, all the frames are jointly modelled.)</li> <li>Here, you can give <code>STARRED</code> the positions of your point sources. Since your images are plate solved, just open the best seeing frame with <code>ds9</code> and measure their coordinates. You only have to do it once.</li> <li>If you are using this as a pipeline, you might want to do the reduction manually once, save the resulting pixelated background in a <code>.fits</code> or <code>.npy</code> file, and provide it here. </li> <li>Whether the pipeline attempts to further refine the background. In a scenario where new frames are incoming regularly, I would set <code>false</code> for this one provided that you did provide a good quality background just above. I would set <code>true</code> with manual supervision, if the aim is set more on the fitted high-resolution model than auto-updating light curves.</li> <li>These values should mostly work. Take a look at the loss curve in the plots after running the pipeline to make sure the optimization converged.</li> <li>Since our images are already plate solved, this will only execute the post plate solving steps.</li> </ol>"},{"location":"tutorial/#introduction","title":"Introduction","text":"<p>By default, <code>lightcurver</code> is a pipeline which executes all its steps sequentially,  through the <code>lightcurver.pipeline.workflow_manager.WorkflowManager</code> class. So, the most basic python script executing the pipeline would be as follows:</p> <pre><code>import os\nos.environ['LIGHTCURVER_CONFIG'] = \"/path/to/config.yaml\"\n\nfrom lightcurver.pipeline.workflow_manager import WorkflowManager\n\nif __name__ == \"__main__\":\n    wf_manager = WorkflowManager()\n    wf_manager.run()\n</code></pre> <p>Where the <code>config.yaml</code> file needs to be carefully tuned before execution. You should always start from  this template.</p> <p>You can also run the script above directly from the command line with the following command: <pre><code>lc_run /path/to/config.yaml\n</code></pre></p> <p><code>lc_run -h</code> will print the following list of steps performed by the pipeline: <pre><code>read_convert_skysub_character_catalog\nplate_solving\ncalculate_common_and_total_footprint\nquery_gaia_for_stars\nstamp_extraction\npsf_modeling\nstar_photometry\ncalculate_normalization_coefficient\ncalculate_absolute_zeropoints\nprepare_calibrated_cutouts\nmodel_calibrated_cutouts\n</code></pre> The pipeline is incremental, but in certain cases it might be useful to start or stop the pipeline at given steps,  for example: this will run the extraction of the cutouts and the modelling of the PSF only. <pre><code>lc_run /path/to/config.yaml --start stamp_extraction --stop psf_modeling\n</code></pre> This will only run the modelling of the ROI: <pre><code>lc_run /path/to/config.yaml --start model_calibrated_cutouts\n</code></pre></p> <p>In this tutorial, to understand the role of the different steps,  we will first execute each step manually rather than executing the pipeline through the <code>WorkflowManager</code>.</p> <p>I will provide you with some wide field images that you can use to follow along. Note the following:</p> <ul> <li>The images are already plate solved, such that you will not need to install <code>Astrometry.net</code> on your computer. Your real life examples will most likely not be, so you should consider installing it.</li> <li>Things will be excruciatingly slow if you do not have a GPU. I would consider using only 4-5 frames in this case.</li> </ul> <p>You can work in a jupyter notebook or just write python scripts with the commands we will execute below.</p> <p>Running lightcurver function calls in python scripts</p> <p>On Mac and Windows computers, you would need to wrap all your function calls in a <code>if __name__ == \"__main__\"</code> block.  These are omitted in the code snippets below for brevity, but you will get an error due to the way multiprocessing spawns processes on Mac and Windows if you don't.</p>"},{"location":"tutorial/#preparing-the-working-directory-and-data","title":"Preparing the working directory and data","text":"<p>The example dataset consists of a few frames from the monitoring of a lensed quasar with the VLT Survey Telescope (VST). You can find it at this link, but we will download it below anyway. Start by creating a working directory. I will assume the working directory <code>/scratch/lightcurver_tutorial</code>, please replace this with your own. <pre><code>export workdir='/scratch/lightcurver_tutorial'\nmkdir $workdir\n</code></pre> Let us store our raw data in this working directory as well for the sake of the example, but of course it could be anywhere, including on a network drive. <pre><code>cd $workdir\nwget https://www.astro.unige.ch/~dux/vst_dataset_example.zip\nunzip vst_dataset_example.zip\n</code></pre> Your data will now be at <code>/scratch/lightcurver_tutorial/raw</code>, here is what a single frame looks like, with the region of interest marked: </p> <p>The pipeline also expects a function able to read the header of your fits files. Store the following python function: <pre><code>def parse_header(header):\n    from dateutil import parser\n    from astropy.time import Time\n    exptime = header['exptime']\n    gain = header['gain']\n    time = Time(parser.parse(header['obstart']))\n    return {'exptime': exptime, 'gain': gain, 'mjd': time.mjd}\n</code></pre> in this file: <code>$workdir/header_parser/parse_header.py</code>.  The pipeline expects to find this file at this exact location relative to your working directory. You will need to adapt the function to your own fits files, the point is: you must return a dictionary of the same structure as the one seen above. </p> <p>Units of your data</p> <p>We will use the <code>exptime</code> and <code>gain</code> information to convert the images to electrons per second, assuming that the starting unit is ADU.  Please adapt the values you return within this function should your units be different.</p> <p>Time units</p> <p>We correct for the proper motion of stars when extracting cutouts later in the pipeline, so you need to stick to providing time information (<code>mjd</code>) as Modified Julian Days.</p> <p>Now, we need to set up the configuration file of the pipeline. This file could be anywhere, but we will put it in our working directory.  I provide a fairly generic configuration  which works well for this particular dataset. Paste the contents of the file in <code>$workdir/config.yaml</code>. You will most probably need to adapt these lines at least: <pre><code>workdir: /scratch/lightcurver_tutorial \n# ...\nraw_dirs:\n  - /scratch/lightcurver_tutorial/raw\n# further below ...\nalready_plate_solved: true\n</code></pre> This last line informs the pipeline about the plate solved status of our files. You can also read through the configuration file to learn about the different options of the pipeline.</p> <p>At any point, you could just run the code block at the very beginning of this page, and the pipeline would likely run to the end, producing an <code>hdf5</code> file with calibrated cutouts and PSFs of our region of interest. We will keep executing each step separately however, so you get a chance to look at the outputs.</p>"},{"location":"tutorial/#initializing-database-and-frame-importation","title":"Initializing database and frame importation","text":"<p>Now would be a good time to fire up a jupyter notebook, each code block below being a new cell. You first need to add the location of your config file to the environment, then you can start executing tasks: <pre><code>import os\n# replace with your actual path:\nos.environ['LIGHTCURVER_CONFIG'] = \"/scratch/lightcurver_tutorial/config.yaml\"\n\nfrom lightcurver.structure.user_config import get_user_config\nfrom lightcurver.structure.database import initialize_database\nfrom lightcurver.pipeline.task_wrappers import read_convert_skysub_character_catalog\n\ninitialize_database()\nread_convert_skysub_character_catalog()\n</code></pre></p> <p>This last command will read all the frames, convert them to electron / second (we are assuming ADU as initial units), subtract the sky, look for sources in the image, calculate ephemeris and finally store everything in our database, at <code>/scratch/lightcurver_tutorial/database.sqlite3</code>.</p> <p>Database</p> <p>You may query the database at any time to understand what is going on. For example, at the moment we have: <pre><code> $ sqlite3 $workdir/database.sqlite3 \"select count(*) from frames\"\n 87\n</code></pre> The database contains the frames, and later will contain stars, links between stars and frames, and more.</p>"},{"location":"tutorial/#plate-solving-and-footprint-calculation","title":"Plate solving and footprint calculation","text":"<p>Even though we started with plate solved images, we are still going to call the plate solving routine. No actual plate solving will take place, but the footprint of each image will be inserted in the database, and we will calculate the total and common footprint to all images. This can be useful if you want to make sure that you are always going to use the same reference stars, in each frame.  Let us go ahead and run the task:</p> <pre><code># Assuming the path to the config file is still in the environment.\nfrom lightcurver.pipeline.task_wrappers import plate_solve_all_frames, calc_common_and_total_footprint_and_save\nplate_solve_all_frames() # (7)\ncalc_common_and_total_footprint_and_save()\n</code></pre> <p>This will have populated the <code>footprints</code> and <code>combined_footprint</code> tables of the database.</p> <p>Footprint shenanigans</p> <p>All downstream steps from this one are linked to a hash value of the combined footprint. If your Gaia stars selection of the next section is made with the <code>ROI_disk</code> strategy, the hash value bypasses the actual footprint and is simply set to the radius of the disk. Thus, adding new frames will not  trigger the reprocessing of everything (but changing the radius will).</p> <p>At this point, you can open the <code>footprints.jpg</code> diagnostic plot which might look something like the following.</p> <p></p> <p>Note that the pipeline eliminates the pointings (sets <code>ROI_in_footprint = 0</code>) in the frames table of the database)  that do not contain your region of interest, these are not shown in the diagnostic plot.</p>"},{"location":"tutorial/#querying-stars-from-gaia","title":"Querying stars from Gaia","text":"<p>Back to the configuration file, I recommend using <pre><code>star_selection_strategy: 'ROI_disk'\nROI_disk_radius_arcseconds: 300 (1)\n</code></pre></p> <p>Next, depending on your data, you will need to adjust the acceptable magnitude range (to include stars that are bright enough while not saturating the sensor.) If you have good seeing and are working with oversampled data, I recommend sticking to a relatively low value of <code>star_max_astrometric_excess_noise</code>. Gaia can sometimes mistake a galaxy for a star, and a galaxy would do no good to your PSF model. Keeping the astrometric excess noise low (e.g., below 3-4) largely reduces the risk of selecting a galaxy. This is how this part is executed: <pre><code># assuming the path to the config file is still in the environment\nfrom lightcurver.processes.star_querying import query_gaia_stars\nquery_gaia_stars()\n</code></pre> This will populate the <code>stars</code> and <code>stars_in_frames</code> tables of the database. The latter allows us to query which star is available in which frame.</p> <p>The plot you can look at to make sure things look reasonable is <code>footprints_with_gaia_stars.jpg</code>, which might look something like this:</p> <p></p>"},{"location":"tutorial/#extraction-of-cutouts","title":"Extraction of cutouts","text":"<p>Now that we've identified stars, let us extract them from the image. This step will</p> <ul> <li>extract the cutouts,</li> <li>compute a noisemap (from the background noise, and photon noise estimation given that we can convert our data to electrons),</li> <li>clean the cosmics (unless stated otherwise in the config),</li> </ul> <p>and those for each selected star, and also for our region of interest. These will all go into the <code>regions.h5</code> file, at the root of the working directory. This is this step is called: <pre><code># assuming the path to the config file is still in the environment\nfrom lightcurver.processes.cutout_making import extract_all_stamps\nextract_all_stamps()\n</code></pre></p> <p>Here is a snippet to see what the cutouts look like: <pre><code>import h5py\nimport matplotlib.pyplot as plt\n\nwith h5py.File('regions.h5', 'r') as f:\n    frames = f['frames']  # main set is called frames\n    frame = frames[list(f['frames'].keys())[1]]  # listing the frames and picking one.\n    data = frame['data']  # looking at data, but you can also go for `mask` or `noisemap`\n    objs = sorted(data.keys())\n    # keep just the last 5\n    objs = objs[-5:]\n    fig, axs = plt.subplots(1, len(objs), figsize=(10, 2))\n    for obj, ax in zip(objs, axs.flatten()):\n        ax.imshow(data[obj], origin='lower')\n        ax.axis('off')\nplt.tight_layout()\nplt.show()\n</code></pre> </p>"},{"location":"tutorial/#modelling-the-psf","title":"Modelling the PSF","text":"<p>This is the most expensive step of the pipeline. For each frame, we are going to simultaneously fit a grid of pixels to all the selected stars. The grid of pixels being regulated by starlets, we delegate the heavy  lifting to <code>STARRED</code>. I recommend sticking to a subsampling factor of 2 unless you have good reasons to go beyond this. You can expect the process to last 2-3 seconds per frame on a middle range gaming GPU, including the loading of the data, the modelling, the plotting, and database update.</p> <pre><code># assuming the path to the config file is still in the environment\nfrom lightcurver.processes.psf_modelling import model_all_psfs\nmodel_all_psfs()\n</code></pre> <p>This will populate the <code>PSFs</code> table in the database, saving the subsampling factor, the reduced chi-squared of the fit, and some text reminding which stars were used to compute the model. You can check the plots at <code>$workdir/plots/PSFs/</code>, here is an example:</p> <p></p> <p>The plot shows all the stars that contributed to the PSF models and their noisemaps.  The last row shows the fit residuals after subtraction of the model, in units of the noise. The last column shows the loss curve of the fit, as well as the PSF model. You might want to skim through some of the PSF plots to make sure there isn't something fishy. If you notice that a star is causing problems in particular, you can exclude it by defining what stars the PSF model can use in the config file. For example, say we need to eliminate star <code>a</code> in the plot above, we'd set: <pre><code>stars_to_use_psf: bcdefghijklmnop\n</code></pre> Then you'd have to re-run the PSF process with <code>redo_psf: true</code>.</p>"},{"location":"tutorial/#psf-photometry-of-the-reference-stars","title":"PSF photometry of the reference stars","text":"<p>This step will, for each star</p> <ul> <li>select which frames contain this star</li> <li>eliminate frames with a poorly fit PSF (looking at the reduced chi-squared values, check the config file for how this is done)</li> <li>jointly fit the PSF to the star in question in all the selected frames.</li> </ul> <p><pre><code># assuming the path to the config file is still in the environment\nfrom lightcurver.processes.star_photometry import do_star_photometry\ndo_star_photometry()\n</code></pre> The fitted fluxes will be saved in the <code>star_flux_in_frame</code> table, together with, again, a reduced chi-squared value that will be used downstream to eliminate the badly fitted frames. Again it is a good idea to check the diagnostic plot, one of which is generated per star. </p> <p>From left to right, we have the mean (stack) of all the cutouts of that stared that went into the PSF photometry, the stacked residuals after subtraction from the fitted model, once in data and once in noise units, the loss curve, and the distribution of reduced chi-squared values of the fit on individual frames.</p>"},{"location":"tutorial/#calculating-a-normalization-coefficient","title":"Calculating a normalization coefficient","text":"<p>This step leverages all the extracted star fluxes, and scales them as to minimize the scatter of the fluxes of different stars in overlaping frames. Once this is done, the fluxes available in each frame will be averaged with sigma-clipping rejection. The average will be taken as the \"normalization coefficient\", and the residual scatter as the uncertainty on the coefficient.</p> <p><pre><code># assuming the path to the config file is still in the environment\nfrom lightcurver.processes.normalization_calculation import calculate_coefficient\ncalculate_coefficient()\n</code></pre> This process fills in the <code>normalization_coefficients</code> table. You can check that the normalization is indeed appropriately flattening the curves of your reference star in the diagnostic plot:</p> <p></p> <p>At the top the normalization coefficient, per frame, plotted in function of the frame. At the bottom the light curve of one of the reference stars.</p>"},{"location":"tutorial/#calculating-zero-points-and-preparing-calibrated-cutouts-of-our-region-of-interest","title":"Calculating zero points and preparing calibrated cutouts of our region of interest","text":"<p>All the heavy lifting having been done, we can use our Gaia stars to estimate the absolute zero point of our images. You can also specify another survey to use for this calibration in the config file.</p> <p><pre><code># assuming the path to the config file is still in the environment\nfrom lightcurver.processes.absolute_zeropoint_calculation import calculate_zeropoints\n\ncalculate_zeropoints()\n</code></pre> Next, we use our normalization coefficient to prepare the calibrated cutouts. <pre><code># assuming the path to the config file is still in the environment\nfrom lightcurver.processes.roi_file_preparation import prepare_roi_file\nprepare_roi_file()\n</code></pre> You will find your calibrated cutouts in the <code>prepared_roi_cutouts</code>, relative to the working directory.</p>"},{"location":"tutorial/#modelling-the-roi","title":"Modelling the ROI","text":"<p>The last and most satisfying step! As a reminder, <code>STARRED</code> jointly models all your epochs at once. It does so by modelling the data as the sum of point sources (whose flux can vary from epoch to epoch) and a pixelated background regularized by wavelets. This part is highly non-linear, and you could follow one of the <code>STARRED</code> tutorials to model your cutouts yourself. Nevertheless, the pipeline does have a modelling step that works for simple cases, so we can also take a look at the available parameters.</p> Simple cases: pipeline scriptManual modellingCombining the two <p>(Check the annotation buttons for comments)</p> <p><pre><code>do_ROI_model: true (2)\npoint_sources: (3)\n   A: [42.202991, 19.225400]\n   B: [42.202944, 19.225186]\n   C: [42.203227, 19.225010]\n   D: [42.203249, 19.225389]\n\n# so, null or a path for this one (path either relative to workdir, or absolute path starting with /):\nstarting_background: null (4)\n# if null above, and false here, then you will not include a background (kinda ruining the point of the entire pipeline, but well...)\nfurther_optimize_background: true (5)\n\n# and these should mostly work as is, how many iterations of the optimizer do we do?\nroi_deconv_translations_iters: 300 (6)\nroi_deconv_all_iters: 2000\n# keep in mind that this is going to be relatively slow on a CPU. (a few minutes at least). Count 30min for the whole pipeline.\n</code></pre> When you are satisfied with the settings, run the modelling step: <pre><code>  from lightcurver.processes.roi_modelling import do_modelling_of_roi\n  do_modelling_of_roi()\n</code></pre></p> <p>And now, in the <code>prepared_roi_cutouts</code> directory you should have </p> <ul> <li>a <code>csv</code> file containing the fluxes and uncertainties of each point source at each epoch, as well as additional information (MJD, seeing, reduced chi-squared, zeropoint so you can convert the fluxes to magnitudes, database ID of the frame),</li> <li>a <code>json</code> file containing the astrometry of the point sources,</li> <li>two fits files containing the fitted high resolution model product, once with point sources and once with the background only.</li> </ul> <p>For more control over the point sources and background, we can manually conduct a <code>STARRED</code> modelling on the cutouts we prepared. Please see this example, which is quite versatile.</p> <p>The idea is to setup the right modelling steps once, for example in a <code>jupyter</code> notebook, then save the notebook as a python script. Then, one can simply run the resulting script at the end of every execution of the pipeline.</p>"}]}